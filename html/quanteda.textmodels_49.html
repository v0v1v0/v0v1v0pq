<div class="container">

<table style="width: 100%;"><tr>
<td>textmodel_affinity</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Class affinity maximum likelihood text scaling model</h2>

<h3>Description</h3>

<p><code>textmodel_affinity()</code> implements the maximum likelihood supervised text
scaling method described in Perry and Benoit (2017).
</p>


<h3>Usage</h3>

<pre><code class="language-R">textmodel_affinity(
  x,
  y,
  exclude = NULL,
  smooth = 0.5,
  ref_smooth = 0.5,
  verbose = quanteda_options("verbose")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>the dfm or bootstrap_dfm
object on which the model will be fit.  Does not need to contain only the
training documents, since the index of these will be matched automatically.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>vector of training classes/scores associated with each document
identified in <code>data</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>exclude</code></td>
<td>
<p>a set of words to exclude from the model</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>smooth</code></td>
<td>
<p>a smoothing parameter for class affinities; defaults to 0.5
(Jeffreys prior). A plausible alternative would be 1.0 (Laplace prior).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ref_smooth</code></td>
<td>
<p>a smoothing parameter for token distributions;
defaults to 0.5</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>logical; if <code>TRUE</code> print diagnostic information during
fitting.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A <code>textmodel_affinity</code> class list object, with elements:
</p>

<ul>
<li> <p><code>smooth</code> a numeric vector of length two for the smoothing parameters <code>smooth</code>
and <code>ref_smooth</code>
<code>x</code> the input model matrix <code>x</code>
<code>y</code> the vector of class training labels <code>y</code>
<code>p</code> a feature <code class="reqn">\times</code> class sparse matrix of estimated class affinities
</p>
</li>
<li> <p><code>support</code> logical vector indicating whether a feature was included in computing
class affinities
</p>
</li>
<li> <p><code>call</code> the model call
</p>
</li>
</ul>
<h3>Author(s)</h3>

<p>Patrick Perry and Kenneth Benoit
</p>


<h3>References</h3>

<p>Perry, P.O. &amp; Benoit, K.R. (2017). Scaling Text with the Class
Affinity Model. <a href="https://doi.org/10.48550/arXiv.1710.08963">doi:10.48550/arXiv.1710.08963</a>.
</p>


<h3>See Also</h3>

<p><code>predict.textmodel_affinity()</code> for methods of applying a
fitted <code>textmodel_affinity()</code> model object to predict quantities from
(other) documents.
</p>


<h3>Examples</h3>

<pre><code class="language-R">(af &lt;- textmodel_affinity(quanteda::data_dfm_lbgexample, y = c("L", NA, NA, NA, "R", NA)))
predict(af)
predict(af, newdata = quanteda::data_dfm_lbgexample[6, ])

## Not run: 
# compute bootstrapped SEs
dfmat &lt;- quanteda::bootstrap_dfm(data_corpus_dailnoconf1991, n = 10, remove_punct = TRUE)
textmodel_affinity(dfmat, y = c("Govt", "Opp", "Opp", rep(NA, 55)))

## End(Not run)
</code></pre>


</div>