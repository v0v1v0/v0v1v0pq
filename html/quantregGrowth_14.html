<div class="container">

<table style="width: 100%;"><tr>
<td>ps</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Specifying a smooth term in the gcrq formula.
</h2>

<h3>Description</h3>

<p>Function used to define the smooth term (via P-splines) within the gcrq formula. 
The function actually does not evaluate a (spline) smooth, but simply it 
passes relevant information to proper fitter functions.
</p>


<h3>Usage</h3>

<pre><code class="language-R">ps(..., lambda = -1, d = 3, by=NULL, ndx = NULL, deg = 3, knots=NULL,
    monotone = 0, concave = 0, var.pen = NULL, pen.matrix=NULL, dropc=TRUE, 
    center=TRUE, K=NULL, decom=FALSE, constr.fit=TRUE, shared.pen=FALSE, 
    st=FALSE, ad=0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>

<p>The covariate supposed to have a nonlinear relationship with the quantile curve(s) being estimated. A B-spline is built, and a (difference) penalty is applied. In growth charts this variable is typically the age. 
If the covariate is a factor, category-specific coefficients are estimated subject to a lasso penalty. See the last example in ?gcrq. A matrix of (continuous) covariates can be also supplied to perfom variable selection (among its columns).
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>

<p>A supplied smoothing parameter for the smooth term. If it is negative scalar, the smoothing parameter is estimated iteratively as discussed in Muggeo et al. (2021). If a positive scalar, it represents the actual smoothing parameter. If it is a vector, cross validation is performed to select the ‘best’ value. See Details in <code>gcrq</code>. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>d</code></td>
<td>

<p>The difference order of the penalty. Default to 3 Ignored if <code>pen.matrix</code> is supplied. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>by</code></td>
<td>

<p>if different from <code>NULL</code>, a numeric or factor variable of the same dimension as the covariate in <code>...</code> If numeric the elements multiply the smooth (i.e. a varying coefficient model); if factor, a smooth is fitted for each factor level. Usually the variable <code>by</code> is also included as main effect in the formula, see examples in <code>gcrq</code>. When <code>by</code> includes a factor, the formula should include the model intecept, i.e. <code>y~g+ps(x,by=g)</code> and not <code>y~ 0+g+ps(x,by=g)</code>. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ndx</code></td>
<td>

<p>The number of intervals of the covariate range used to build the B-spline basis. Non-integer values are rounded by <code>round()</code>. If <code>NULL</code>, default, it is taken <code class="reqn">min(n/4,9)</code> (versions &lt;=1.1-0 it was <code class="reqn">min(n/4,40)</code>, the empirical rule of Ruppert). It could be reduced further (but no less than 5 or 6, say) if the sample size is not large and the default value leads to some error in the fitting procedure, see section <code>Note</code> in <code>gcrq</code>. Likewise, if the underlying relationship is strongly nonlinear, <code>ndx</code> could be increased. The returned basis wil have '<code>ndx</code>+<code>deg</code>-<code>1</code>' (if <code>dropc=TRUE</code>) basis functions.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>deg</code></td>
<td>

<p>The degree of the spline polynomial. Default to 3. The B-spline basis is composed by <code>ndx</code>+<code>deg</code> basis functions and if <code>dropc=TRUE</code> the first column is removed for identifiability (and the model intercept is estimated without any penalty). 

</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>knots</code></td>
<td>

<p>The knots locations. If <code>NULL</code>, equispaced knots are set. Note if predictions outside the observed covariate range have to be computed (via <code>predict.gcrq</code>), the knots should be set enought outside the observed range. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>monotone</code></td>
<td>

<p>Numeric value to set up monotonicity restrictions on the first derivative of fitted smooth function
</p>

<ul>
<li>
<p>  '0' = no constraint (default);
</p>
</li>
<li>
<p>  '1' = non-decreasing smooth function;
</p>
</li>
<li>
<p> '-1' = non-increasing smooth function.
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>concave</code></td>
<td>

<p>Numeric value to set up monotonicity restrictions on the second derivative of fitted smooth function
</p>

<ul>
<li>
<p>  '0' = no constraint (default);
</p>
</li>
<li>
<p>  '1' = concave smooth function;
</p>
</li>
<li>
<p> '-1' = convex smooth function.
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>var.pen</code></td>
<td>

<p>A character indicating the varying penalty. See Details.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pen.matrix</code></td>
<td>

<p>if provided, a penalty matrix <code class="reqn">A</code>, say, such that the penalty in the objective function, apart from the smoothing parameter, is <code class="reqn">||Ab||_1</code> where <code class="reqn">b</code> is the spline coefficient vector being penalized.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dropc</code></td>
<td>

<p>logical. Should the first column of the B-spline basis be dropped for the basis identifiability? Default to <code>TRUE</code>. Note, if <code>dropc=FALSE</code> is set, 
it is necessary to omit the model intercept AND not to center the basis, i.e. <code>center=FALSE</code>. Alternatively, both a full basis and the model intercept may be included by adding a small ridge penalty via <code>lambda.ridge&gt;0</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>center</code></td>
<td>

<p>logical. If <code>TRUE</code> the smooth effects are 'centered' over the covariate values, i.e. <code class="reqn">\sum_i \hat{f}(x_i)=0</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>K</code></td>
<td>

<p>A scalar tuning the selection of wiggliness of the smoothed curve when <code class="reqn">\lambda</code> has to be estimated (i.e. <code>lambda&lt;0</code> is set). The larger <code>K</code>, the smoother the curve. Simulations suggest <code>K=2</code> for the smoothing, and <code>K=log(n/p^(2/3))</code> for variable selection and random intercepts (<code>p</code> is the number of variables or number of subjects). See details.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>decom</code></td>
<td>

<p>logical. If <code>TRUE</code>, the B-spline <code class="reqn">B</code> (with a <code class="reqn">d</code> order difference penalty) is decomposed into truncated power functions namely unpenalized polynomial terms up to degree d-1, and additional terms <code class="reqn">Z= B D'(DD')^{-1}</code>. Only the coefficients of <code class="reqn">Z</code> are penalized via an identity matrix, i.e. a lasso penalty. Currently <code>decom=TRUE</code> does not work with shape (monotonicity and concavity) restrictions and noncrossing constraints.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>constr.fit</code></td>
<td>

<p>logical. If <code>monotone</code> or <code>concave</code> are different from 0, <code>constr.fit=TRUE</code> means that these constraints are set on the fitted quantiles rather than on the spline coefficients.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>shared.pen</code></td>
<td>

<p>logical. If <code>TRUE</code> and the smooth is a VC term with a factor specified in <code>by</code>, the smooths in each level of the factor share the same smoothing parameter.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>st</code></td>
<td>

<p>logical. If <code>TRUE</code> the variable(s) are standardized via the <code>scale()</code> function. Typically used for variable selection via lasso, i.e. when a matrix of covariates is passed in <code>ps()</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ad</code></td>
<td>

<p>a positive number to carry out a form of <em>adaptive</em> lasso. More specifically, at each step of the iterative algorithm, the penalty is <code class="reqn">\lambda\sum_jw_j|\beta_j|</code> where <code class="reqn">w_j=|\tilde{\beta}_j|^\mathtt{-ad}</code> and <code class="reqn">\tilde{\beta}_j</code> are estimates coming from the previous iteration with a different value of  <code class="reqn">\lambda</code>. <code>ad=0</code> means the standard lasso and <code>ad=1</code> adaptive lasso (with weights updated during the iterative process. 
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>If a numeric variable has been supplied, <code>ps()</code> builds a B-spline basis with number of columns equal to <code>ndx+deg</code> (or <code>length(knots)-deg-1</code>). However, unless <code>dropc=FALSE</code> is specified, the first column is removed for identifiability, and the spline coefficients are penalized via differences of order <code>d</code>; <code>d=0</code> leads to a penalty on the coefficients themselves.  If <code>pen.matrix</code> is supplied, <code>d</code> is ignored. Since versions 1.5-0 and 1.6-0, a factor or matrix can be supplied.
</p>
<p><code>lambda</code> is the tuning parameter, fixed or to be estimated. When <code>lambda</code>=0 an unpenalized (and typically wiggly) fit is obtained, and as lambda increases the curve gets smoother till a <code>d-1</code> degree polynomial. At 'intermediate' lambda values, the fitted curve is a <em>piecewise</em> polynomial of degree <code>d-1</code>.
</p>
<p>It is also possible to put a varying penalty via the argument <code>var.pen</code>. Namely for a 
constant smoothing (<code>var.pen=NULL</code>) the penalty is <code class="reqn">\lambda\sum_k |\Delta^d_k|</code> where 
<code class="reqn">\Delta^d_k</code> is the k-th difference (of order <code>d</code>) of the spline coefficients. For instance if <code class="reqn">d=1</code>, 
<code class="reqn">|\Delta^1_k|=|b_k-b_{k-1}|</code> where the <code class="reqn">b_k</code>  are the spline coefficients.
When a varying penalty is set, the penalty becomes <code class="reqn">\lambda\sum_k |\Delta_k^d| w_k</code> where the weights <code class="reqn">w_k</code> depend on <code>var.pen</code>; for instance <code>var.pen="((1:k)^2)"</code> results in <code class="reqn">w_k=k^2</code>. See models <code>m6</code> and <code>m6a</code> in the examples of <code>gcrq</code>.
</p>
<p>If <code>decom=TRUE</code>, the smooth can be plotted with or without the fixed part, see <code>overall.eff</code> in the function <code>plot.gcrq</code>.
</p>


<h3>Value</h3>

<p>The function simply returns the covariate with added attributes relevant to 
smooth term.
</p>


<h3>Note</h3>

<p>For shape-constrained fits, use <code>constr.fit=FALSE</code> only if you are using a single full and uncentred basis, namely something like <br><code>gcrq(y~0+ps(x, center=FALSE, dropc=FALSE, monotone=1, constr.fit=FALSE),..)</code>.
</p>


<h3>Author(s)</h3>

<p>Vito M. R. Muggeo
</p>


<h3>References</h3>

<p>Muggeo VMR, Torretta F, Eilers PHC, Sciandra M, Attanasio M (2021).
Multiple smoothing parameters selection in additive regression quantiles,
<em>Statistical Modelling</em>, 21, 428-448.
</p>
<p>For a general discussion on using B-spline and penalties in regression model see
</p>
<p>Eilers PHC, Marx BD. (1996) Flexible smoothing with B-splines and penalties. 
Statistical Sciences, 11:89-121.
</p>


<h3>See Also</h3>

<p><code>gcrq</code>, <code>plot.gcrq</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">##see ?gcrq

##gcrq(y ~ ps(x),..) #it works (default: center = TRUE, dropc = TRUE)
##gcrq(y ~ 0 + ps(x, center = TRUE, dropc = FALSE)) #it does NOT work
##gcrq(y ~ 0 + ps(x, center = FALSE, dropc = FALSE)) #it works

</code></pre>


</div>