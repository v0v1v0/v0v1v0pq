<div class="container">

<table style="width: 100%;"><tr>
<td>rqss</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Additive Quantile Regression Smoothing</h2>

<h3>Description</h3>

<p>Fitting function for additive quantile regression models with possible univariate
and/or bivariate nonparametric terms estimated by total variation regularization.
See <code>summary.rqss</code> and <code>plot.rqss</code> for further details on inference and
confidence bands.
</p>


<h3>Usage</h3>

<pre><code class="language-R">rqss(formula, tau = 0.5, data = parent.frame(), weights, subset, na.action,
	method = "sfn", lambda = NULL, contrasts = NULL, ztol = 1e-5, control, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p>  a formula object, with the response on the left of a ‘~’
operator,  and terms, separated by ‘+’ operators, on the right.
The terms may include <code>qss</code> terms that represent additive
nonparametric components.  These terms can be univariate or
bivariate.  See <code>qss</code> for details on how to
specify these terms.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tau</code></td>
<td>

<p>the quantile to be estimated, this must be a number between 0 and 1,
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>

<p>a data.frame in which to interpret the variables
named in the formula, or in the subset and the weights argument.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>

<p>vector of observation weights; if supplied, the algorithm fits
to minimize the sum of the weights multiplied into the
absolute residuals. The length of weights must be the same as
the number of observations.  The weights must be nonnegative
and it is strongly recommended that they be strictly positive,
since zero weights are ambiguous.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>subset</code></td>
<td>
<p>an optional vector specifying a subset of observations to be
used in the fitting.  This can be a vector of indices of observations
to be included, or a logical vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>na.action</code></td>
<td>

<p>a function to filter missing data.
This is applied to the model.frame after any subset argument has been used.
The default (with <code>na.fail</code>) is to create an error if any missing values are
found.  A possible alternative is <code>na.omit</code>, which
deletes observations that contain one or more missing values.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>

<p>the algorithmic method used to compute the fit.  There are currently
two options.   Both are implementations of the Frisch–Newton interior
point method described in detail in Portnoy and Koenker(1997).   Both
are implemented using sparse Cholesky decomposition as described in
Koenker and Ng (2003).
</p>
<p>Option <code>"sfnc"</code> is used if the user specifies inequality constraints.
Option <code>"sfn"</code> is used if there are no inequality constraints.
Linear inequality constraints on the fitted coefficients are specified
by a matrix <code>R</code> and a vector <code>r</code>, specified inside the <code>qss</code>
terms, representing the constraints in the form <code class="reqn">Rb \ge r</code>.
</p>
<p>The option <code>method = "lasso"</code> allows one to penalize the coefficients
of the covariates that have been entered linearly as in <code>rq.fit.lasso</code>;
when this is specified then there should be an additional <code>lambda</code>
argument specified that determines the amount of shrinkage.  
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p> can be either a scalar, in which case all the slope coefficients 
are assigned this value, or alternatively, the user can specify a vector of length 
equal to the number of linear covariates plus one (for the intercept) and these
values will be used as coordinate dependent shrinkage factors.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>contrasts</code></td>
<td>

<p>a list giving contrasts for some or all of the factors
default = <code>NULL</code> appearing in the model formula.
The elements of the list should have the same name as the variable
and should be either a contrast matrix (specifically, any full-rank
matrix with as many rows as there are levels in the factor),
or else a function to compute such a matrix given the number of levels.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ztol</code></td>
<td>
<p>A zero tolerance parameter used to determine the number of
zero residuals in the fitted object which in turn determines the effective
dimensionality of the fit.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>control</code></td>
<td>
<p> control argument for the fitting routines
(see <code>sfn.control</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Other arguments passed to fitting routines</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p> Total variation regularization for univariate and
bivariate nonparametric quantile smoothing is described
in Koenker, Ng and Portnoy (1994) and Koenker and Mizera(2003)
respectively.  The additive model extension of this approach
depends crucially on the sparse linear algebra implementation
for R described in Koenker and Ng (2003).  There are extractor
methods <code>logLik</code> and <code>AIC</code> that is
relevant to lambda selection.  A more detailed description of
some recent developments of these methods is available from
within the package with <code>vignette("rq")</code>.  Since this
function uses sparse versions of the interior point algorithm
it may also prove to be useful for fitting linear models
without <code>qss</code> terms when the design has a sparse
structure, as for example when there is a complicated factor 
structure.  
</p>
<p>If the <span class="pkg">MatrixModels</span> and <span class="pkg">Matrix</span> packages are both  loadable then the 
linear-in-parameters portion of the design matrix is made in sparse matrix form;
this is helpful in large applications with many factor variables for which dense 
formation of the design matrix would take too much space.
</p>
<p>Although modeling with <code>rqss</code> typically imposes smoothing penalties on
the total variation of the first derivative, or gradient, of the fitted functions,
for univariate smoothing, it is also possible to penalize total variation of
the function itself using the option <code>Dorder = 0</code> inside <code>qss</code> terms.
In such cases, estimated functions are piecewise constant rather than piecewise
linear.  See the documentation for <code>qss</code> for further details.
</p>


<h3>Value</h3>

<p>The function returns a fitted object representing the estimated
model specified in the formula.  See <code>rqss.object</code>
for further details on this object, and references to methods
to look at it.
</p>


<h3>Note</h3>

<p>If you intend to embed calls to <code>rqss</code> inside another function, then
it is advisable to pass a data frame explicitly as the <code>data</code> argument
of the <code>rqss</code> call, rather than relying on the magic of R scoping rules.
</p>


<h3>Author(s)</h3>

<p> Roger Koenker </p>


<h3>References</h3>

<p>[1] Koenker, R. and S. Portnoy (1997)
The Gaussian Hare and the Laplacean
Tortoise:  Computability of Squared-error vs Absolute Error Estimators,
(with discussion).
<em>Statistical Science</em> <b>12</b>, 279–300.
</p>
<p>[2] Koenker, R., P. Ng and S. Portnoy, (1994)
Quantile Smoothing Splines;
<em>Biometrika</em> <b>81</b>, 673–680.
</p>
<p>[3] Koenker, R. and I. Mizera, (2003)
Penalized Triograms: Total Variation Regularization for Bivariate Smoothing;
<em>JRSS(B)</em> <b>66</b>, 145–163.
</p>
<p>[4] Koenker, R. and P. Ng (2003)
SparseM:  A Sparse Linear Algebra Package for R,
<em>J. Stat. Software</em>.
</p>


<h3>See Also</h3>

 <p><code>qss</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">n &lt;- 200
x &lt;- sort(rchisq(n,4))
z &lt;- x + rnorm(n)
y &lt;- log(x)+ .1*(log(x))^2 + log(x)*rnorm(n)/4 + z
plot(x, y-z)
f.N  &lt;- rqss(y ~ qss(x, constraint= "N") + z)
f.I  &lt;- rqss(y ~ qss(x, constraint= "I") + z)
f.CI &lt;- rqss(y ~ qss(x, constraint= "CI") + z)

lines(x[-1], f.N $coef[1] + f.N $coef[-(1:2)])
lines(x[-1], f.I $coef[1] + f.I $coef[-(1:2)], col="blue")
lines(x[-1], f.CI$coef[1] + f.CI$coef[-(1:2)], col="red")

## A bivariate example
if(requireNamespace("interp")){
if(requireNamespace("interp")){
data(CobarOre)
fCO &lt;- rqss(z ~ qss(cbind(x,y), lambda= .08), data=CobarOre)
plot(fCO)
}}</code></pre>


</div>