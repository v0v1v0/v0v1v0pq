<div class="container">

<table style="width: 100%;"><tr>
<td>textmodel_lsa</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Latent Semantic Analysis</h2>

<h3>Description</h3>

<p>Fit the Latent Semantic Analysis scaling model to a dfm,
which may be weighted (for instance using <code>quanteda::dfm_tfidf()</code>).
</p>


<h3>Usage</h3>

<pre><code class="language-R">textmodel_lsa(x, nd = 10, margin = c("both", "documents", "features"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>the dfm on which the model will be fit</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nd</code></td>
<td>
<p>the number of dimensions to be included in output</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>margin</code></td>
<td>
<p>margin to be smoothed by the SVD</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>svds in the <span class="pkg">RSpectra</span> package is applied to
enable the fast computation of the SVD.
</p>


<h3>Value</h3>

<p>a <code>textmodel_lsa</code> class object, a list containing:
</p>

<ul>
<li> <p><code>sk</code> a numeric vector containing the d values from the SVD
</p>
</li>
<li> <p><code>docs</code> document coordinates from the SVD (u)
</p>
</li>
<li> <p><code>features</code> feature coordinates from the SVD (v)
</p>
</li>
<li> <p><code>matrix_low_rank</code> the multiplication of udv'
</p>
</li>
<li> <p><code>data</code> the input data as a CSparseMatrix from the <span class="pkg">Matrix</span> package
</p>
</li>
</ul>
<h3>Note</h3>

<p>The number of dimensions <code>nd</code> retained in LSA is an empirical
issue. While a reduction in <code class="reqn">k</code> can remove much of the noise, keeping
too few dimensions or factors may lose important information.
</p>


<h3>Author(s)</h3>

<p>Haiyan Wang and Kohei Watanabe
</p>


<h3>References</h3>

<p>Rosario, B. (2000).
<a href="http://www.cse.msu.edu/~cse960/Papers/LSI/LSI.pdf">Latent Semantic Indexing: An Overview</a>. <em>Technical report INFOSYS 240 Spring
Paper, University of California, Berkeley.</em>
</p>
<p>Deerwester, S., Dumais, S.T., Furnas, G.W., Landauer, T.K., &amp;
Harshman, R. (1990). <a href="https://www.proquest.com/docview/1301252034">Indexing by Latent Semantic Analysis</a>. <em>Journal of the American Society for
Information Science</em>, 41(6): 391.
</p>


<h3>See Also</h3>

<p><code>predict.textmodel_lsa()</code>, <code>coef.textmodel_lsa()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">library("quanteda")
dfmat &lt;- dfm(tokens(data_corpus_irishbudget2010))
# create an LSA space and return its truncated representation in the low-rank space
tmod &lt;- textmodel_lsa(dfmat[1:10, ])
head(tmod$docs)

# matrix in low_rank LSA space
tmod$matrix_low_rank[,1:5]

# fold queries into the space generated by dfmat[1:10,]
# and return its truncated versions of its representation in the new low-rank space
pred &lt;- predict(tmod, newdata = dfmat[11:14, ])
pred$docs_newspace

</code></pre>


</div>