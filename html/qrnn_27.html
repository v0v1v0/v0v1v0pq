<div class="container">

<table style="width: 100%;"><tr>
<td>qrnn-package</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Quantile Regression Neural Network</h2>

<h3>Description</h3>

<p>This package implements the quantile regression neural network (QRNN)
(Taylor, 2000; Cannon, 2011; Cannon, 2018), which is a flexible nonlinear form
of quantile regression. While low level modelling functions are available, it is
recommended that the <code>mcqrnn.fit</code> and <code>mcqrnn.predict</code>
wrappers be used for most applications. More information is provided below.
</p>
<p>The goal of quantile regression is to estimate conditional quantiles of a
response variable that depend on covariates in some form of regression equation.
The QRNN adopts the multi-layer perceptron neural network architecture. The
implementation follows from previous work on the estimation of censored
regression quantiles, thus allowing predictions for mixed discrete-continuous
variables like precipitation (Friederichs and Hense, 2007). A differentiable
approximation to the quantile regression cost function is adopted so that a
simplified form of the finite smoothing algorithm (Chen, 2007) can be used to
estimate model parameters. This approximation can also be used to force the
model to solve a standard least squares regression problem or an expectile
regression problem (Cannon, 2018). Weight penalty regularization can be added
to help avoid overfitting, and ensemble models with bootstrap aggregation are
also provided.
</p>
<p>An optional monotone constraint can be invoked, which guarantees monotonic
non-decreasing behaviour of model outputs with respect to specified covariates
(Zhang, 1999). The input-hidden layer weight matrix can also be constrained
so that model relationships are strictly additive (see <code>gam.style</code>;
Cannon, 2018). Borrowing strength by using a composite model for multiple
regression quantiles (Zou et al., 2008; Xu et al., 2017) is also possible
(see <code>composite.stack</code>). Weights can be applied to individual cases
(Jiang et al., 2012).
</p>
<p>Applying the monotone constraint in combination with the composite model allows
one to simultaneously estimate multiple non-crossing quantiles (Cannon, 2018);
the resulting monotone composite QRNN (MCQRNN) is provided by the
<code>mcqrnn.fit</code> and <code>mcqrnn.predict</code> wrapper functions.
Examples for <code>qrnn.fit</code> and <code>qrnn2.fit</code> show how the
same functionality can be achieved using the low level
<code>composite.stack</code> and fitting functions.
</p>
<p>QRNN models with a single layer of hidden nodes can be fitted using the
<code>qrnn.fit</code> function. Predictions from a fitted model are made using
the <code>qrnn.predict</code> function. The function <code>gam.style</code>
can be used to visualize and investigate fitted covariate/response relationships
from <code>qrnn.fit</code> (Plate et al., 2000). Note: a single hidden layer
is usually sufficient for most modelling tasks. With added monotonicity
constraints, a second hidden layer may sometimes be beneficial
(Lang, 2005; Minin et al., 2010). QRNN models with two hidden layers are
available using the <code>qrnn2.fit</code> and
<code>qrnn2.predict</code> functions. For non-crossing quantiles, the
<code>mcqrnn.fit</code> and <code>mcqrnn.predict</code> wrappers also allow
models with one or two hidden layers to be fitted and predictions to be made
from the fitted models.
</p>
<p>In general, <code>mcqrnn.fit</code> offers a convenient, single function for 
fitting multiple quantiles simultaneously. Note, however, that default 
settings in <code>mcqrnn.fit</code> and other model fitting functions are 
not optimized for general speed, memory efficiency, or accuracy and should be 
adjusted for a particular regression problem as needed. In particular, the 
approximation to the quantile regression cost function <code>eps.seq</code>, the 
number of trials <code>n.trials</code>, and number of iterations <code>iter.max</code> 
can all influence fitting speed (and accuracy), as can changing the 
optimization algorithm via <code>method</code>. Non-crossing quantiles are 
implemented by stacking multiple copies of the <code>x</code> and <code>y</code> data, 
one copy per value of <code>tau</code>. Depending on the dataset size, this can 
lead to large matrices being passed to the optimization routine. In the 
<code>adam</code> adaptive stochastic gradient descent method, the
<code>minibatch</code> size can be adjusted to help offset this cost. Model complexity
is determined via the number of hidden nodes, <code>n.hidden</code> and
<code>n.hidden2</code>, as well as the optional weight penalty <code>penalty</code>; values
of these hyperparameters are crucial to obtaining a well performing model.
</p>
<p>When using <code>mcqrnn.fit</code>, it is also possible to estimate the full
quantile regression process by specifying a single integer value for <code>tau</code>.
In this case, <code>tau</code> is the number of random samples used in the stochastic
estimation. For more information, see Tagasovska and Lopez-Paz (2019). It may be
necessary to restart the optimization multiple times from the previous weights
and biases, in which case <code>init.range</code> can be set to the <code>weights</code>
values from the previously completed optimization run. For large datasets, it is
recommended that the <code>adam</code> method with an appropriate integer
<code>tau</code> and <code>minibatch</code> size be used for optimization.
</p>
<p>If models for multiple quantiles have been fitted, for example by
<code>mcqrnn.fit</code> or multiple calls to either <code>qrnn.fit</code>
or <code>qrnn2.fit</code>, the (experimental) <code>dquantile</code>
function and its companion functions are available to create proper
probability density, distribution, and quantile functions
(Quiñonero-Candela et al., 2006; Cannon, 2011). Alternative distribution,
quantile, and random variate functions based on the Nadaraya-Watson estimator
(Passow and Donner, 2020) are also available in <code>[p,q,r]quantile.nw</code>.
These can be useful for assessing probabilistic calibration and evaluating
model performance.
</p>
<p>Note: the user cannot easily change the output layer transfer function
to be different than <code>hramp</code>, which provides either the identity function or a
ramp function to accommodate optional left censoring. Some applications, for
example fitting smoothed binary quantile regression models for a binary target
variable (Kordas, 2006), require an alternative like the logistic sigmoid.
While not straightforward, it is possible to change the output layer transfer
function by switching off <code>scale.y</code> in the call to the fitting
function and reassigning <code>hramp</code> and <code>hramp.prime</code> as follows:
</p>
<pre>
library(qrnn)

# Use the logistic sigmoid as the output layer transfer function
To.logistic &lt;- function(x, lower, eps) 0.5 + 0.5*tanh(x/2)
environment(To.logistic) &lt;- asNamespace("qrnn")
assignInNamespace("hramp", To.logistic, ns="qrnn")

# Change the derivative of the output layer transfer function
To.logistic.prime &lt;- function(x, lower, eps) 0.25/(cosh(x/2)^2)
environment(To.logistic.prime) &lt;- asNamespace("qrnn")
assignInNamespace("hramp.prime", To.logistic.prime, ns="qrnn")

</pre>


<h3>Details</h3>


<table>
<tr>
<td style="text-align: left;">
Package: </td>
<td style="text-align: left;"> qrnn</td>
</tr>
<tr>
<td style="text-align: left;">
Type: </td>
<td style="text-align: left;"> Package</td>
</tr>
<tr>
<td style="text-align: left;">
License: </td>
<td style="text-align: left;"> GPL-2</td>
</tr>
<tr>
<td style="text-align: left;">
LazyLoad: </td>
<td style="text-align: left;"> yes</td>
</tr>
<tr>
<td style="text-align: left;">
</td>
</tr>
</table>
<h3>References</h3>

<p>Cannon, A.J., 2011. Quantile regression neural networks: implementation
in R and application to precipitation downscaling. Computers &amp; Geosciences,
37: 1277-1284. doi:10.1016/j.cageo.2010.07.005
</p>
<p>Cannon, A.J., 2018. Non-crossing nonlinear regression quantiles by
monotone composite quantile regression neural network, with application
to rainfall extremes. Stochastic Environmental Research and Risk Assessment,
32(11): 3207-3225. doi:10.1007/s00477-018-1573-6
</p>
<p>Chen, C., 2007. A finite smoothing algorithm for quantile regression.
Journal of Computational and Graphical Statistics, 16: 136-164.
</p>
<p>Friederichs, P. and A. Hense, 2007. Statistical downscaling of extreme
precipitation events using censored quantile regression. Monthly Weather
Review, 135: 2365-2378. 
</p>
<p>Jiang, X., J. Jiang, and X. Song, 2012. Oracle model selection for nonlinear
models based on weighted composite quantile regression. Statistica Sinica,
22(4): 1479-1506.
</p>
<p>Kordas, G., 2006. Smoothed binary regression quantiles. Journal of Applied
Econometrics, 21(3): 387-407.
</p>
<p>Lang, B., 2005. Monotonic multi-layer perceptron networks as universal
approximators. International Conference on Artificial Neural Networks,
Artificial Neural Networks: Formal Models and Their Applications-ICANN 2005,
pp. 31-37.
</p>
<p>Minin, A., M. Velikova, B. Lang, and H. Daniels, 2010. Comparison of universal
approximators incorporating partial monotonicity by structure.
Neural Networks, 23(4): 471-475.
</p>
<p>Passow, C., R.V. Donner, 2020. Regression-based distribution mapping for
bias correction of climate model outputs using linear quantile regression.
Stochastic Environmental Research and Risk Assessment, 34: 87-102.
</p>
<p>Plate, T., J. Bert, J. Grace, and P. Band, 2000. Visualizing the function
computed by a feedforward neural network. Neural Computation,
12(6): 1337-1354.
</p>
<p>Quiñonero-Candela, J., C. Rasmussen, F. Sinz, O. Bousquet,
B. Scholkopf, 2006. Evaluating Predictive Uncertainty Challenge.
Lecture Notes in Artificial Intelligence, 3944: 1-27.
</p>
<p>Tagasovska, N., D. Lopez-Paz, 2019. Single-model uncertainties for deep
learning. Advances in Neural Information Processing Systems, 32,
NeurIPS 2019. doi:10.48550/arXiv.1811.00908
</p>
<p>Taylor, J.W., 2000. A quantile regression neural network approach to
estimating the conditional density of multiperiod returns. Journal of
Forecasting, 19(4): 299-311.
</p>
<p>Xu, Q., K. Deng, C. Jiang, F. Sun, and X. Huang, 2017. Composite quantile
regression neural network with applications. Expert Systems with Applications,
76, 129-139.
</p>
<p>Zhang, H. and Zhang, Z., 1999. Feedforward networks with monotone
constraints. In: International Joint Conference on Neural Networks,
vol. 3, p. 1820-1823. doi:10.1109/IJCNN.1999.832655
</p>
<p>Zou, H. and M. Yuan, 2008. Composite quantile regression and the oracle model
selection theory. The Annals of Statistics, 1108-1126.
</p>


</div>