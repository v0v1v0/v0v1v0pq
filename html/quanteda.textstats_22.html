<div class="container">

<table style="width: 100%;"><tr>
<td>textstat_collocations</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Identify and score multi-word expressions</h2>

<h3>Description</h3>

<p>Identify and score multi-word expressions, or adjacent fixed-length
collocations, from text.
</p>


<h3>Usage</h3>

<pre><code class="language-R">textstat_collocations(
  x,
  method = "lambda",
  size = 2,
  min_count = 2,
  smoothing = 0.5,
  tolower = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>a character, corpus, or
tokens object whose collocations will be scored.  The
tokens object should include punctuation, and if any words have been
removed, these should have been removed with <code>padding = TRUE</code>. While
identifying collocations for tokens objects is supported, you will get
better results with character or corpus objects due to relatively imperfect
detection of sentence boundaries from texts already tokenized.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>association measure for detecting collocations. Currently this
is limited to <code>"lambda"</code>.  See Details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>size</code></td>
<td>
<p>integer; the length of the collocations
to be scored</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>min_count</code></td>
<td>
<p>numeric; minimum frequency of collocations that will be
scored</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>smoothing</code></td>
<td>
<p>numeric; a smoothing parameter added to the observed counts
(default is 0.5)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tolower</code></td>
<td>
<p>logical; if <code>TRUE</code>, form collocations as lower-cased
combinations</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>additional arguments passed to tokens()</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Documents are grouped for the purposes of scoring, but collocations will not
span sentences. If <code>x</code> is a tokens object and some tokens
have been removed, this should be done using <code style="white-space: pre;">⁠[tokens_remove](x, pattern, padding = TRUE)⁠</code> so that counts will still be accurate, but the pads will
prevent those collocations from being scored.
</p>
<p>The <code>lambda</code> computed for a size = <code class="reqn">K</code>-word target multi-word expression
the coefficient for the  <code class="reqn">K</code>-way interaction parameter in the saturated
log-linear model fitted to the counts of the terms forming the set of
eligible multi-word expressions. This is the same as the "lambda" computed in
Blaheta and Johnson's (2001), where all multi-word expressions are considered
(rather than just verbs, as in that paper). The <code>z</code> is the Wald
<code class="reqn">z</code>-statistic computed as the quotient of <code>lambda</code> and the Wald statistic
for <code>lambda</code> as described below.
</p>
<p>In detail:
</p>
<p>Consider a <code class="reqn">K</code>-word target expression <code class="reqn">x</code>, and let <code class="reqn">z</code> be any
<code class="reqn">K</code>-word expression. Define a comparison function <code class="reqn">c(x,z)=(j_{1},
\dots, j_{K})=c</code> such that the <code class="reqn">k</code>th element of <code class="reqn">c</code> is 1 if the
<code class="reqn">k</code>th word in <code class="reqn">z</code> is equal to the <code class="reqn">k</code>th word in <code class="reqn">x</code>, and 0
otherwise. Let <code class="reqn">c_{i}=(j_{i1}, \dots, j_{iK})</code>, <code class="reqn">i=1, \dots,
2^{K}=M</code>, be the possible values of <code class="reqn">c(x,z)</code>, with <code class="reqn">c_{M}=(1,1,
\dots, 1)</code>. Consider the set of <code class="reqn">c(x,z_{r})</code> across all expressions
<code class="reqn">z_{r}</code> in a corpus of text, and let <code class="reqn">n_{i}</code>, for <code class="reqn">i=1,\dots,M</code>,
denote the number of the <code class="reqn">c(x,z_{r})</code> which equal <code class="reqn">c_{i}</code>, plus the
smoothing constant <code>smoothing</code>. The <code class="reqn">n_{i}</code> are the counts in a
<code class="reqn">2^{K}</code> contingency table whose dimensions are defined by the
<code class="reqn">c_{i}</code>.
</p>
<p><code class="reqn">\lambda</code>: The <code class="reqn">K</code>-way interaction parameter in the saturated
loglinear model fitted to the <code class="reqn">n_{i}</code>. It can be calculated as
</p>
<p style="text-align: center;"><code class="reqn">\lambda  = \sum_{i=1}^{M} (-1)^{K-b_{i}} * log n_{i}</code>
</p>

<p>where <code class="reqn">b_{i}</code> is the number of the elements of <code class="reqn">c_{i}</code> which are
equal to 1.
</p>
<p>Wald test <code class="reqn">z</code>-statistic <code class="reqn">z</code> is calculated as:
</p>
<p style="text-align: center;"><code class="reqn">z = \frac{\lambda}{[\sum_{i=1}^{M} n_{i}^{-1}]^{(1/2)}}</code>
</p>



<h3>Value</h3>

<p><code>textstat_collocations</code> returns a data.frame of collocations and
their scores and statistics. This consists of the collocations, their
counts, length, and <code class="reqn">\lambda</code> and <code class="reqn">z</code> statistics.  When <code>size</code> is a
vector, then <code>count_nested</code> counts the lower-order collocations that occur
within a higher-order collocation (but this does not affect the
statistics).
</p>


<h3>Author(s)</h3>

<p>Kenneth Benoit, Jouni Kuha, Haiyan Wang, and Kohei Watanabe
</p>


<h3>References</h3>

<p>Blaheta, D. &amp; Johnson, M. (2001). <a href="http://web.science.mq.edu.au/~mjohnson/papers/2001/dpb-colloc01.pdf">Unsupervised learning of multi-word verbs</a>.
Presented at the ACLEACL Workshop on the Computational Extraction, Analysis
and Exploitation of Collocations.
</p>


<h3>Examples</h3>

<pre><code class="language-R">library("quanteda")
corp &lt;- data_corpus_inaugural[1:2]
head(cols &lt;- textstat_collocations(corp, size = 2, min_count = 2), 10)
head(cols &lt;- textstat_collocations(corp, size = 3, min_count = 2), 10)

# extracting multi-part proper nouns (capitalized terms)
toks1 &lt;- tokens(data_corpus_inaugural)
toks2 &lt;- tokens_remove(toks1, pattern = stopwords("english"), padding = TRUE)
toks3 &lt;- tokens_select(toks2, pattern = "^([A-Z][a-z\\-]{2,})", valuetype = "regex",
                       case_insensitive = FALSE, padding = TRUE)
tstat &lt;- textstat_collocations(toks3, size = 3, tolower = FALSE)
head(tstat, 10)

# vectorized size
txt &lt;- c(". . . . a b c . . a b c . . . c d e",
         "a b . . a b . . a b . . a b . a b",
         "b c d . . b c . b c . . . b c")
textstat_collocations(txt, size = 2:3)

# compounding tokens from collocations
toks &lt;- tokens("This is the European Union.")
colls &lt;- tokens("The new European Union is not the old European Union.") %&gt;%
    textstat_collocations(size = 2, min_count = 1, tolower = FALSE)
colls
tokens_compound(toks, colls, case_insensitive = FALSE)

#' # from a collocations object
(coll &lt;- textstat_collocations(tokens("a b c a b d e b d a b")))
phrase(coll)
</code></pre>


</div>