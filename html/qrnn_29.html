<div class="container">

<table style="width: 100%;"><tr>
<td>qrnn.fit</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Main function used to fit a QRNN model or ensemble of QRNN models
</h2>

<h3>Description</h3>

<p>Function used to fit a QRNN model or ensemble of QRNN models.
</p>


<h3>Usage</h3>

<pre><code class="language-R">qrnn.fit(x, y, n.hidden, w=NULL, tau=0.5, n.ensemble=1,
         iter.max=5000, n.trials=5, bag=FALSE, lower=-Inf,
         init.range=c(-0.5, 0.5, -0.5, 0.5), monotone=NULL,
         additive=FALSE, eps.seq=2^seq(-8, -32, by=-4),
         Th=sigmoid, Th.prime=sigmoid.prime, penalty=0,
         unpenalized=NULL, n.errors.max=10, trace=TRUE,
         scale.y=TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>

<p>covariate matrix with number of rows equal to the number of samples and number of columns equal to the number of variables.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>

<p>response column matrix with number of rows equal to the number of samples.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.hidden</code></td>
<td>

<p>number of hidden nodes in the QRNN model.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>w</code></td>
<td>

<p>vector of weights with length equal to the number of samples;
<code>NULL</code> gives equal weight to each sample.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tau</code></td>
<td>

<p>desired tau-quantile(s).
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.ensemble</code></td>
<td>

<p>number of ensemble members to fit.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iter.max</code></td>
<td>

<p>maximum number of iterations of the optimization algorithm.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.trials</code></td>
<td>

<p>number of repeated trials used to avoid local minima.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bag</code></td>
<td>

<p>logical variable indicating whether or not bootstrap aggregation (bagging) should be used.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lower</code></td>
<td>

<p>left censoring point.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>init.range</code></td>
<td>

<p>initial weight range for input-hidden and hidden-output weight matrices.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>monotone</code></td>
<td>

<p>column indices of covariates for which the monotonicity constraint should hold.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>additive</code></td>
<td>

<p>force additive relationships.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps.seq</code></td>
<td>

<p>sequence of <code>eps</code> values for the finite smoothing algorithm.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Th</code></td>
<td>

<p>hidden layer transfer function; use <code>sigmoid</code>, <code>elu</code>, <code>relu</code>, <code>lrelu</code>, 
<code>softplus</code>, or other non-decreasing function for a nonlinear model and <code>linear</code> for a linear model.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Th.prime</code></td>
<td>

<p>derivative of the hidden layer transfer function <code>Th</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>penalty</code></td>
<td>

<p>weight penalty for weight decay regularization.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>unpenalized</code></td>
<td>

<p>column indices of covariates for which the weight penalty should not be applied to input-hidden layer weights.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.errors.max</code></td>
<td>

<p>maximum number of <code>nlm</code> optimization failures allowed before quitting.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>trace</code></td>
<td>

<p>logical variable indicating whether or not diagnostic messages are printed during optimization.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>scale.y</code></td>
<td>

<p>logical variable indicating whether <code>y</code> should be scaled to zero mean and unit standard deviation.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>

<p>additional parameters passed to the <code>nlm</code> optimization routine.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Fit a censored quantile regression neural network model for the
<code>tau</code>-quantile by minimizing a cost function based on smooth
Huber-norm approximations to the tilted absolute value and ramp functions.
Left censoring can be turned on by setting <code>lower</code> to a value
greater than <code>-Inf</code>. A simplified form of the finite smoothing
algorithm, in which the <code>nlm</code> optimization algorithm
is run with values of the <code>eps</code> approximation tolerance progressively
reduced in magnitude over the sequence <code>eps.seq</code>, is used to set the
QRNN weights and biases. Local minima of the cost function can be
avoided by setting <code>n.trials</code>, which controls the number of
repeated runs from different starting weights and biases, to a value
greater than one.
</p>
<p>(Note: if <code>eps.seq</code> is set to a single, sufficiently large value and <code>tau</code>
is set to <code>0.5</code>, then the result will be a standard least squares
regression model. The same value of <code>eps.seq</code> and other values
of <code>tau</code> leads to expectile regression.)
</p>
<p>If invoked, the <code>monotone</code> argument enforces non-decreasing behaviour
between specified columns of <code>x</code> and model outputs. This holds if
<code>Th</code> and <code>To</code> are monotone non-decreasing functions. In this case,
the <code>exp</code> function is applied to the relevant weights following
initialization and during optimization; manual adjustment of
<code>init.weights</code> or <code>qrnn.initialize</code> may be needed due to
differences in scaling of the constrained and unconstrained weights.
Non-increasing behaviour can be forced by transforming the relevant
covariates, e.g., by reversing sign.
</p>
<p>The <code>additive</code> argument sets relevant input-hidden layer weights
to zero, resulting in a purely additive model. Interactions between covariates
are thus suppressed, leading to a compromise in flexibility between
linear quantile regression and the quantile regression neural network.
</p>
<p>Borrowing strength by using a composite model for multiple regression quantiles
is also possible (see <code>composite.stack</code>). Applying the monotone
constraint in combination with the composite model allows
one to simultaneously estimate multiple non-crossing quantiles;
the resulting monotone composite QRNN (MCQRNN) is demonstrated in
<code>mcqrnn</code>.
</p>
<p>In the linear case, model complexity does not depend on the number
of hidden nodes; the value of <code>n.hidden</code> is ignored and is instead
set to one internally. In the nonlinear case, <code>n.hidden</code>
controls the overall complexity of the model. As an added means of
avoiding overfitting, weight penalty regularization for the magnitude
of the input-hidden layer weights (excluding biases) can be applied
by setting <code>penalty</code> to a nonzero value. (For the linear model,
this penalizes both input-hidden and hidden-output layer weights,
leading to a quantile ridge regression model. In this case, kernel
quantile ridge regression can be performed with the aid of the
<code>qrnn.rbf</code> function.) Finally, if the <code>bag</code> argument
is set to <code>TRUE</code>, models are trained on bootstrapped <code>x</code> and
<code>y</code> sample pairs; bootstrap aggregation (bagging) can be turned
on by setting <code>n.ensemble</code> to a value greater than one. Averaging
over an ensemble of bagged models will also tend to alleviate
overfitting.
</p>
<p>The <code>gam.style</code> function can be used to plot modified
generalized additive model effects plots, which are useful for visualizing
the modelled covariate-response relationships.
</p>
<p>Note: values of <code>x</code> and <code>y</code> need not be standardized or
rescaled by the user. All variables are automatically scaled to zero
mean and unit standard deviation prior to fitting and parameters are
automatically rescaled by <code>qrnn.predict</code> and other prediction
functions. Values of <code>eps.seq</code> are relative to the residuals in
standard deviation units. Note: scaling of <code>y</code> can be turned off using
the <code>scale.y</code> argument.
</p>


<h3>Value</h3>

<p>a list containing elements
</p>
<table>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>a list containing fitted weight matrices</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lower</code></td>
<td>
<p>left censoring point</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps.seq</code></td>
<td>
<p>sequence of <code>eps</code> values for the finite smoothing algorithm</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tau</code></td>
<td>
<p>desired tau-quantile(s)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Th</code></td>
<td>
<p>hidden layer transfer function</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x.center</code></td>
<td>
<p>vector of column means for <code>x</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x.scale</code></td>
<td>
<p>vector of column standard deviations for <code>x</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y.center</code></td>
<td>
<p>vector of column means for <code>y</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y.scale</code></td>
<td>
<p>vector of column standard deviations for <code>y</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>monotone</code></td>
<td>
<p>column indices indicating covariate monotonicity constraints.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>additive</code></td>
<td>
<p>force additive relationships.</p>
</td>
</tr>
</table>
<h3>References</h3>

<p>Cannon, A.J., 2011. Quantile regression neural networks: implementation
in R and application to precipitation downscaling. Computers &amp; Geosciences,
37: 1277-1284. doi:10.1016/j.cageo.2010.07.005
</p>
<p>Cannon, A.J., 2018. Non-crossing nonlinear regression quantiles by
monotone composite quantile regression neural network, with application
to rainfall extremes. Stochastic Environmental Research and Risk Assessment,
32(11): 3207-3225. doi:10.1007/s00477-018-1573-6
</p>


<h3>See Also</h3>

<p><code>qrnn.predict</code>, <code>qrnn.cost</code>, <code>composite.stack</code>, <code>mcqrnn</code>, <code>gam.style</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">x &lt;- as.matrix(iris[,"Petal.Length",drop=FALSE])
y &lt;- as.matrix(iris[,"Petal.Width",drop=FALSE])

cases &lt;- order(x)
x &lt;- x[cases,,drop=FALSE]
y &lt;- y[cases,,drop=FALSE]

tau &lt;- c(0.05, 0.5, 0.95)
 
set.seed(1)

## QRNN models for conditional 5th, 50th, and 95th percentiles
w &lt;- p &lt;- vector("list", length(tau))
for(i in seq_along(tau)){
    w[[i]] &lt;- qrnn.fit(x=x, y=y, n.hidden=3, tau=tau[i],
                       iter.max=200, n.trials=1)
    p[[i]] &lt;- qrnn.predict(x, w[[i]])
}

## Monotone composite QRNN (MCQRNN) for simultaneous estimation of
## multiple non-crossing quantile functions
x.y.tau &lt;- composite.stack(x, y, tau)
fit.mcqrnn &lt;- qrnn.fit(cbind(x.y.tau$tau, x.y.tau$x), x.y.tau$y,
                       tau=x.y.tau$tau, n.hidden=3, n.trials=1,
                       iter.max=500, monotone=1)
pred.mcqrnn &lt;- matrix(qrnn.predict(cbind(x.y.tau$tau, x.y.tau$x),
                      fit.mcqrnn), ncol=length(tau))

par(mfrow=c(1, 2))
matplot(x, matrix(unlist(p), nrow=nrow(x), ncol=length(p)), col="red",
        type="l")
points(x, y)
matplot(x, pred.mcqrnn, col="blue", type="l")
points(x, y)

</code></pre>


</div>