<div class="container">

<table style="width: 100%;"><tr>
<td>dfm_weight</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Weight the feature frequencies in a dfm</h2>

<h3>Description</h3>

<p>Weight the feature frequencies in a dfm
</p>


<h3>Usage</h3>

<pre><code class="language-R">dfm_weight(
  x,
  scheme = c("count", "prop", "propmax", "logcount", "boolean", "augmented", "logave"),
  weights = NULL,
  base = 10,
  k = 0.5,
  smoothing = 0.5,
  force = FALSE
)

dfm_smooth(x, smoothing = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>document-feature matrix created by dfm</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>scheme</code></td>
<td>
<p>a label of the weight type:
</p>

<dl>
<dt><code>count</code></dt>
<dd>
<p><code class="reqn">tf_{ij}</code>, an integer feature count (default when a dfm
is created)</p>
</dd>
<dt><code>prop</code></dt>
<dd>
<p>the proportion of the feature counts of total feature counts
(aka relative frequency), calculated as <code class="reqn">tf_{ij} / \sum_j tf_{ij}</code></p>
</dd>
<dt><code>propmax</code></dt>
<dd>
<p>the proportion of the feature counts of the highest
feature count in a document, <code class="reqn">tf_{ij} / \textrm{max}_j tf_{ij}</code></p>
</dd>
<dt><code>logcount</code></dt>
<dd>
<p>take the 1 + the logarithm of each count, for the
given base, or 0 if the count was zero: <code class="reqn">1 +
  \textrm{log}_{base}(tf_{ij})</code> if <code class="reqn">tf_{ij} &gt; 0</code>, or 0 otherwise.</p>
</dd>
<dt><code>boolean</code></dt>
<dd>
<p>recode all non-zero counts as 1</p>
</dd>
<dt><code>augmented</code></dt>
<dd>
<p>equivalent to <code class="reqn">k + (1 - k) *</code> <code>dfm_weight(x, "propmax")</code></p>
</dd>
<dt><code>logave</code></dt>
<dd>
<p>(1 + the log of the counts) / (1 + log of the average count
within document), or </p>
<p style="text-align: center;"><code class="reqn">\frac{1 + \textrm{log}_{base} tf_{ij}}{1 +
  \textrm{log}_{base}(\sum_j tf_{ij} / N_i)}</code>
</p>
</dd>
<dt><code>logsmooth</code></dt>
<dd>
<p>log of the counts + <code>smooth</code>, or <code class="reqn">tf_{ij} + s</code></p>
</dd>
</dl>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>if <code>scheme</code> is unused, then <code>weights</code> can be a named
numeric vector of weights to be applied to the dfm, where the names of the
vector correspond to feature labels of the dfm, and the weights will be
applied as multipliers to the existing feature counts for the corresponding
named features.  Any features not named will be assigned a weight of 1.0
(meaning they will be unchanged).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>base</code></td>
<td>
<p>base for the logarithm when <code>scheme</code> is <code>"logcount"</code> or
<code>logave</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>the k for the augmentation when <code>scheme = "augmented"</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>smoothing</code></td>
<td>
<p>constant added to the dfm cells for smoothing, default is 1
for <code>dfm_smooth()</code> and 0.5 for <code>dfm_weight()</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>force</code></td>
<td>
<p>logical; if <code>TRUE</code>, apply weighting scheme even if the dfm
has been weighted before.  This can result in invalid weights, such as as
weighting by <code>"prop"</code> after applying <code>"logcount"</code>, or after
having grouped a dfm using <code>dfm_group()</code>.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p><code>dfm_weight</code> returns the dfm with weighted values.  Note the
because the default weighting scheme is <code>"count"</code>, simply calling this
function on an unweighted dfm will return the same object.  Many users will
want the normalized dfm consisting of the proportions of the feature counts
within each document, which requires setting <code>scheme = "prop"</code>.
</p>
<p><code>dfm_smooth</code> returns a dfm whose values have been smoothed by
adding the <code>smoothing</code> amount. Note that this effectively converts a
matrix from sparse to dense format, so may exceed memory requirements
depending on the size of your input matrix.
</p>


<h3>References</h3>

<p>Manning, C.D., Raghavan, P., &amp; Sch√ºtze, H. (2008).
<em>An Introduction to Information Retrieval</em>. Cambridge: Cambridge University Press.
<a href="https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf">https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf</a>
</p>


<h3>See Also</h3>

<p><code>docfreq()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">dfmat1 &lt;- dfm(tokens(data_corpus_inaugural))

dfmat2 &lt;- dfm_weight(dfmat1, scheme = "prop")
topfeatures(dfmat2)
dfmat3 &lt;- dfm_weight(dfmat1)
topfeatures(dfmat3)
dfmat4 &lt;- dfm_weight(dfmat1, scheme = "logcount")
topfeatures(dfmat4)
dfmat5 &lt;- dfm_weight(dfmat1, scheme = "logave")
topfeatures(dfmat5)

# combine these methods for more complex dfm_weightings, e.g. as in Section 6.4
# of Introduction to Information Retrieval
head(dfm_tfidf(dfmat1, scheme_tf = "logcount"))

# smooth the dfm
dfmat &lt;- dfm(tokens(data_corpus_inaugural))
dfm_smooth(dfmat, 0.5)
</code></pre>


</div>